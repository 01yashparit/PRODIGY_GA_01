# PRODIGY_GA_01
# Prodigy Infotech - GPT-2 Fine-Tuning Project (PRODIGY_GA_01)

# ðŸ“ GPT-2 Text Generation - ProDigy Infotech Task

This repository contains the solution for the text generation task as part of the ProDigy Infotech internship. The project involves fine-tuning a pre-trained GPT-2 model on a custom dataset to generate text that mimics a specific style.

---

## ðŸš€ Project Overview

The core of this project is **fine-tuning**, a process where a large, general-purpose language model (like GPT-2) is further trained on a smaller, specific dataset. This makes the model adept at generating text in the style of that dataset.

-   **Model:** `gpt2` from Hugging Face
-   **Library:** `transformers`, `torch`, `datasets`
-   **Dataset:** A custom text file (`train.txt`) containing short, creative prose.

---

## ðŸ› ï¸ How to Run

1.  **Clone the repository:**
    ```bash
    git clone [your-repo-link]
    cd [your-repo-name]
    ```

2.  **Install dependencies:**
    Make sure you have Python 3.8+ installed. It's recommended to use a virtual environment.
    ```bash
    pip install -r requirements.txt
    ```

3.  **Prepare your data:**
    Place your custom dataset in a file named `train.txt` in the root directory.

4.  **Run the fine-tuning script:**
    This will train the model and save the fine-tuned version in a new directory called `./gpt2-finetuned`.
    ```bash
    python main.py
    ```

5.  **Generate text:**
    Use the fine-tuned model to generate new text based on a prompt.
    ```bash
    python generate.py
    ```
6.  **Use app.py**
    Use this app to local host this project as website or public server web site

    ```bash
     streamlit run app.py
    ```


## âœ¨ Sample Output

Here's an example of text generated by the model after fine-tuning.

**Prompt:**
> The world is a canvas

**Generated Text:**
> The world is a canvas, a brushstroke of light in the endless dark. Every star is a story, every shadow a forgotten dream. We listen to the silence and find the universe whispering back.

## âœ… Author
**Yash Parit**  
[GitHub](https://github.com/01yashparit) | [LinkedIn](https://linkedin.com/in/yash-parit01)
